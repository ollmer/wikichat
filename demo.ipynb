{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec79d766-939a-451a-bb6c-7e1478df6e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/conda/envs/wikichat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "from augmented_llm import AugmentedLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be90ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q6_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 18 (mostly Q6_K)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2176.26 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 12176 MB\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "llm = AugmentedLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba82ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Augmentation refers to technologies that are designed to enhance or improve the abilities of humans. This can include a wide range of different technologies, such as exoskeletons, prosthetics, and neural implants. The goal of these technologies is typically to help people perform tasks that they would otherwise be unable to do, whether due to physical limitations or other factors.\n",
      "\n",
      "One example of human augmentation technology is the use of exoskeletons to assist people with mobility issues. These devices can provide additional support and strength to a person's limbs, allowing them to walk more easily and without as much pain. Another example is the use of neural implants to help people with neurological disorders or injuries regain lost functions.\n",
      "\n",
      "Overall, human augmentation technologies have the potential to greatly improve the quality of life for many people. However, there are also concerns about the safety and ethics of these technologies, particularly when it comes to issues like privacy and autonomy.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.ask(\"What is the human augmentation?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
